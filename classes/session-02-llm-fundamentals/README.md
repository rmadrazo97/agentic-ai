# Session 2: LLM Fundamentals & Prompt Engineering ğŸ§ 

**Duration**: 80 minutes | **Time**: 8:30 - 9:50

---

## ğŸ¯ Session Goals
- Understand how LLMs actually work under the hood
- Master core prompt engineering patterns
- Choose the right model for your use case
- Build your first production-ready prompt lab

---

## â±ï¸ Session Timeline

### 1. Stand-up #1 (10 min)

#### ğŸ“Š Team Progress Format
**1-line per team**: Yesterday's progress â†’ Today's target â†’ Blocker

```markdown
Team Alpha: âœ… Set up repo structure â†’ ğŸ¯ Basic agent skeleton â†’ ğŸš§ API key setup
Team Beta: âœ… Chose domain (EdTech) â†’ ğŸ¯ Research RAG patterns â†’ ğŸš§ None
Team Gamma: âœ… Team formation â†’ ğŸ¯ Project charter â†’ ğŸš§ Need help with scope
```

**Post as comments on your team's Draft PR**

---

### 2. Micro-Lecture 1: "How an LLM Works" (15 min)

#### ğŸ” The Magic Revealed

##### **Tokenization** - Words â†’ Numbers
```python
# Example: "Hello world!" becomes [15496, 1917, 0]
import tiktoken
encoder = tiktoken.get_encoding("cl100k_base")
tokens = encoder.encode("Hello world!")
print(f"Tokens: {tokens}")  # [15496, 1917, 0]
print(f"Count: {len(tokens)}")  # 3 tokens
```

##### **Context Windows** - Your Memory Budget
| Model | Context Limit | Real Cost Impact |
|-------|---------------|------------------|
| GPT-4o-mini | 128K tokens | ~$0.60 per full context |
| Claude 3.5 Haiku | 200K tokens | ~$1.00 per full context |
| Gemini 2.0 Flash | 1M tokens | ~$7.50 per full context |

##### **Sampling** - Creativity vs Consistency
```python
# Temperature = 0: Always same output (deterministic)
# Temperature = 0.7: Balanced creativity 
# Temperature = 1.2: Wild and unpredictable
```

#### ğŸ® [Live Token Visualizer â†’](https://platform.openai.com/tokenizer)

---

### 3. Micro-Lecture 2: Prompt Patterns (15 min)

#### ğŸ¯ Evolution of Prompting

##### **Zero-Shot** - Just Ask
```python
prompt = "Summarize this article in 3 bullet points: [ARTICLE]"
```

##### **Few-Shot** - Show Examples
```python
prompt = """
Examples:
Article: "Bitcoin rises 5%..." â†’ â€¢ Bitcoin gains 5% â€¢ Driven by institutional demand â€¢ Market cap hits $1T

Article: "Climate change impacts..." â†’ â€¢ Global temperatures rising â€¢ Extreme weather increasing â€¢ Action needed urgently

Now summarize: [YOUR_ARTICLE]
"""
```

##### **Chain of Thought (CoT)** - Think Step by Step
```python
prompt = "Let's think step by step:\n1. What is the main topic?\n2. What are the key facts?\n3. What's the conclusion?\n\nNow summarize: [ARTICLE]"
```

##### **JSON/Function Calls** - Structured Output
```python
prompt = """
Return JSON with this exact structure:
{
  "summary": "one sentence",
  "key_points": ["point1", "point2", "point3"],
  "sentiment": "positive|neutral|negative"
}
"""
```

#### âš¡ Live Demo Time!
```bash
# Follow along - we'll test each pattern live
cd classes/session-02-llm-fundamentals/labs
python prompt_lab.py --pattern zero
python prompt_lab.py --pattern fewshot --provider anthropic
```

---

### 4. Micro-Lecture 3: Choosing a Model (10 min)

#### ğŸ¤” Decision Framework

##### **Context Need**
- **< 4K tokens**: Any model works
- **< 128K tokens**: GPT-4o, Claude 3.5
- **< 1M tokens**: Gemini 2.0 Pro only

##### **Priority Triangle** (Pick 2 of 3)
```
     SPEED
    /     \
   /       \
  /         \
PRICE ---- ACCURACY
```

##### **Provider Comparison**
| Provider | Best For | Strengths | Cost (per 1M tokens) |
|----------|----------|-----------|---------------------|
| **OpenAI** | General use | Fastest adoption, tools | $2.50 (4o-mini) |
| **Anthropic** | Complex reasoning | Safety, long context | $1.25 (Haiku) |
| **Google** | Multimodal | Huge context, vision | $0.075 (Flash) |
| **X AI** | Real-time data | Live web access | $5.00 (Grok) |

#### ğŸ“‹ [Model Selection Cheat Sheet â†’](./docs/model_selection.md)

---

### 5. Guided Lab: Prompt Engineering CLI (20 min)

#### ğŸ› ï¸ Setup Instructions

```bash
# 1. Navigate to lab directory
cd classes/session-02-llm-fundamentals/labs

# 2. Set up API keys (choose one)
export OPENAI_API_KEY="sk-..."        # GPT models
export ANTHROPIC_API_KEY="sk-ant-..."  # Claude models  
export GOOGLE_API_KEY="AI..."          # Gemini models

# 3. Install dependencies
pip install openai anthropic google-generativeai pyyaml

# 4. Test basic functionality
python prompt_lab.py --help
```

#### ğŸ§ª Lab Tasks

##### **Task 1: Zero-Shot Baseline**
```bash
python prompt_lab.py --pattern zero --provider openai
```

##### **Task 2: Few-Shot Improvement**
```bash
python prompt_lab.py --pattern fewshot --provider anthropic
```

##### **Task 3: JSON Schema Enforcement**
```bash
python prompt_lab.py --pattern json --provider google
```

##### **Task 4: Cost Comparison**
```bash
python prompt_lab.py --pattern fewshot --provider openai --model gpt-4o-mini
python prompt_lab.py --pattern fewshot --provider anthropic --model claude-3-haiku-20240307
```

#### ğŸ“Š Track Your Results
- **Quality**: Which pattern gives best summaries?
- **Speed**: Which provider is fastest?
- **Cost**: Which setup is most economical?

---

### 6. Wrap-up & Next Steps (10 min)

#### âœ… Before You Leave
- [ ] Ran all 4 lab tasks successfully
- [ ] Compared results across providers
- [ ] Committed your best prompt_lab.py
- [ ] Created branch `lab-prompt-eng`

#### ğŸ“š Homework (Due Session 3)

1. **Read**: [Anthropic's Prompt Engineering Guide](https://docs.anthropic.com/claude/docs/introduction-to-prompt-design)
2. **Read**: Chapter 2 of [LLM Agents in Practice](https://www.manning.com/books/llm-agents-in-practice)
3. **Complete**: Lab deliverable (see below)
4. **Optional**: Try [Promptfoo](https://www.promptfoo.dev/) for automated evaluation

#### ğŸ¯ Lab Deliverable

**Push to branch `lab-prompt-eng`:**

1. **Modified `prompt_lab.py`** with:
   - âœ… Working zero-shot implementation
   - âœ… Working few-shot implementation  
   - âœ… Working JSON schema implementation
   - âœ… Cost tracking and comparison

2. **Create `EVAL_REPORT.md`** with:
   - Results table (pattern vs quality score)
   - Cost comparison across providers
   - Reflection: "Which tweak gave the biggest quality jump and why?"

**Auto-grading**: PR must pass CI tests = Lab Checkpoint 1 complete

---

## ğŸ”§ Lab Files Structure

```
labs/
â”œâ”€â”€ prompt_lab.py          # ğŸ‘ˆ Main file you'll edit
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ article.txt        # Sample article to summarize
â”‚   â””â”€â”€ fewshot_examples.json
â”œâ”€â”€ eval/
â”‚   â”œâ”€â”€ rubric.yaml        # Automated test cases
â”‚   â””â”€â”€ run_eval.py        # Evaluation runner
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ model_client.py    # Multi-provider wrapper
â”‚   â””â”€â”€ cost_tracker.py    # Token & cost tracking
â””â”€â”€ docs/
    â””â”€â”€ model_selection.md  # Provider comparison guide
```

---

## ğŸš¨ Quick Fixes for Common Issues

| Problem | Solution |
|---------|----------|
| **"Under-specified output"** | Add format: "Return exactly 3 bullets â‰¤ 12 words each" |
| **"Hallucinated facts"** | Add: "Only use information from the provided text" |
| **"Unstable JSON"** | Use function calling or strict schema validation |
| **"High cost"** | Truncate context, use cheaper models for simple tasks |
| **"High latency"** | Try Claude Haiku or Gemini Flash for speed |

---

## ğŸ‰ Share Your Wins!

**Post your best results in #lab-results Slack:**
- Screenshot of cost comparison
- Your most improved prompt
- Interesting failure cases

---

*Next Session: Tool-Calling Agents & ReAct Patterns*